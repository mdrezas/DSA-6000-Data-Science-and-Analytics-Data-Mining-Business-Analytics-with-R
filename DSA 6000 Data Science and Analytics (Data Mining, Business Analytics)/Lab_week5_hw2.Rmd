---
title: "Lab Exercise Week 5 and Homework 2"
author: "Md Reza"
date: "October 15, 2019"
output: html_document
---

## Requirements
1. Do the following exercises, answer Q1 to Q13, and present your answers in a HTML file generated by RMarkdown. Specifically, the RMarkdown file of this document is provided as a template and starting point, so just finish this document by typing up your answers after each question, and format your answers appropriately. Note that the answers for the first 3 questions are given as an example. 

2. Do exericses ISLR 3.7 Problem 1, 3 and 4. Type up your answers at the end of this document. No need to transcribe the problem descriptions, just clearly label your answers to match with the problem number. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,comment="")
```

## Build a Multiple Linear Regression Model and Assess its Performance

First, load the *MASS* package that contains the Boston dataset. The Boston data set is loaded with package, use `?Boston` to learn about the meanings of each column.
```{r}
library(MASS) 
```

* __Q1__: In the Boston data set, what is the Total Sum of Squares (TSS) of the median house value _medv_? `r sum((Boston$medv - mean(Boston$medv))^2)`


Fit _medv_ (median house value) as a linear function of _lstat_ (percentage of lower status of the population) and _age_ (proportion of units built prior to 1940). 
```{r}
lm.fit <- lm(medv ~ lstat + age, data = Boston)
```

From the model summary, answer these questions:

* __Q2__: What is the Residual Sum of Squares (RSS) of this fit? `r (RSS <- (nrow(Boston)-2-1)*(summary(lm.fit)$sigma)^2)`

* __Q3__: How much proportion of the variance in _medv_ is accounted for by this fit? `r summary(lm.fit)$r.sq`

* __Q4__: Overall, is there a relationship between the response variable _medv_ and the predictors _lstat_ and _age_? How do you make such a conclusion? 

  __Answer__: There is a relationship between the response variable _medv_ and the predictors _lstat_ and _age_ because the larger value of $F$ and the small value of $p$ that suggest that there is a relationship between the predictors and response variable, though $age$ statistically isn't significant.

* __Q5__: Is there a relationship between _medv_ and each of the predictors? How do you tell? 

  __Answer__: The larger value of $F$ and the small value of $p$ that suggest that there is a relationship between the predictors and response variable, though $age$ and $indus$ statistically aren't significant.

* __Q6__: What is the estimated standard deviation of the irreducible error in the model $\text{medv} = \beta_0 + \beta_1 \cdot \text{lstat} + \beta_2 \cdot \text{age} + \epsilon$. 

  __Answer__: $RSE$ is just an estimate of the standard deviation of the irreducible error, which is: $6.173$.

* __Q7__: How do you interpret the coefficient estimate on _age_, i.e., the value `r format(coef(lm.fit)['age'], digits=3)`, in the model context?

  __Answer__: This indicates that an increase in _age_ is associated with an increase in _medv_. To be precise, a one-unit increase in _age_ is associated with an increase in the _medv_ by 0.03454 units.
  However notice that the p-value for _age_ is high, and this also indicates that there is no statistical evidence of a difference in the _medv_ for _age_.


## Build a Full Model and Compute VIFs

Let's first check if all columns of the data set are numeric hence can be used as predictors.

```{r}
sapply(Boston, mode)
```

Are all columns of the numeric type? `r all(sapply(Boston, mode) == "numeric")`. Let's try using all columns (except for _medv_, of course) as predictors and fit a full model. 

```{r}
lm.fit.full <- lm(medv ~ ., data = Boston)
```

Now display the model summary using `summary(lm.fit.full)` and answewr these questions.

* __Q8__: How many predictors are used in this model fit?  $13$ predictors.

* __Q9__: Which predictors have a significant effect on the response variable at a confidence level of 0.95?

    $lstat$ and $rm$ have significant effect on the response variable.

* __Q10__: Does _age_ present a significant effect on _medv_ in this model?  

    $age$ no longer statistically significant.

Let's calculate the Variance Inflation Factor (VIF) of each predictor in the above full model. VIF of a predictor $X_j$ measures how well $X_j$ can be represented as a linear combination of other predictors used in the model. A high VIF indicates that the predictor has a high multicollinearity with other predictors, thus should probably be excluded from the set of predictors to improve the model fit. 
VIF of a predictor $X_j$ is calculated by the formula $\text{VIF}_j = \frac{1}{1-R_j^2}$, where $R_j^2$ is the R-Square value of the model fit by using $X_j$ as the response variable and all other predictors used in the original model as predictors. For instance, to calculate the VIF of _age_ in the context of `lm.fit.full` model:

```{r}
(VIF_age <- 1/(1-summary(lm(age ~ .-medv, data = Boston))$r.sq))
```

Note that in the above line, `lm(age ~ .-medv, data=Boston)` means to fit a linear model for _age_ using all columns of `Boston` but `medv`. 

We can use the `vif` function in the `car` package to calculate the VIF for all predictors in a model, as follows.

```{r}
#install.packages("car")  # run this line if you have not installed the "car" package before
library(car)
vif(lm.fit.full)
```


## Model Selection

  Now, let's exclude _age_ from the full model and fit again. 

  ```{r}
  lm.fit.all_but_age <- lm(medv ~ . - age, data = Boston)
  ```

  Display the `summary(lm.fit.all_but_age)` and answer these questions.

  ```{r}
  library(car)
  vif(lm.fit.full)
  lm.fit.all_but_age <- lm(medv ~ . - age, data = Boston)
  summary(lm.fit.all_but_age)
  ```

*__Q11__: Compare the Multiple R-squared and Adjusted R-sqaured values between this model and the full model. Comment on the differences. Which model do you think is better?

      lm.fit.all_but_age -> Multiple R-squared:  0.7406,	Adjusted R-squared:  0.7343  
      lm.fit.full        -> Multiple R-squared:  0.7406,	Adjusted R-squared:  0.7338

    The fact that excluding "age" from the model leads to a little bit increased in the adjusted R^2, which provides additional evidence that excluding "age" would enhance the model above what would be obtained by probability. Therefore, model "lm.fit.all_but_age" should be leveraged.

*__Q12__: Compute the AIC and BIC of the model `lm.fit.full` and `lm.fit.all_but_age`, respectively. Use the function `AIC(lm.fit.full)` and `BIC(lm.fit.full)`. Compare the two models in terms of AIC and BIC. Does the conclusion consistent with what you get in Q11?

  ```{r}
  AIC(lm.fit.full)
  BIC(lm.fit.full)
  AIC(lm.fit.all_but_age)
  BIC(lm.fit.all_but_age)
  ```

    Yes, the conclusion is consistent because a lower AIC/BIC indicates a better fit, and in this case, the model "lm.fit.all_but_age" has lower AIC/BIC.  

*__Q13__: Pick a termination criteria (i.e., Adj. R-squared, AIC or BIC) and carry out the Backward Selection procedure to find the best fit. 
  
  ```{r}
  fullmodel <- lm(medv~., data = Boston)
  model.step.b<- step(fullmodel,direction='backward')
  ```
*Best fit from $Backward$ Selection procedure*  
  ```{r}
  summary(model.step.b)
  ```
  
## ISLR Exercises (Section 3.7)

### Problem 1:
*Given p-value precisely the null hypothesis should be: $H_0^{(1)} : \beta_1 = 0$, $H_0^{(2)} : \beta_2 = 0$ and $H_0^{(3)} : \beta_3 = 0$. Where per table 3.4 though the p-values for "TV" and "Radio" statistically significant but it's not for "newspaper"; therefore, we conclude that we could reject $H_0^{(1)}$ and $H_0^{(2)}$, but we can not reject $H_0^{(3)}$, because newspaper has minimal to no effect on sales.*


### Problem 3:
__(a)__ 

With the least square method the line we get that best fit the model is: 
\[\hat{y} = 50 + 20GPA + 0.07IQ + 35Gender + 0.01(GPA\times IQ) - 10(GPA\times Gender)\]

 where for males it becomes:
\[\hat{y} = 50 + 20GPA + 0.07IQ + 0.01(GPA\times IQ),\]

 and for females it becomes:
\[\hat{y} = 50 + 20GPA + 0.07IQ + 35 + 0.01(GPA\times IQ) - 10(GPA)\]

Therefore, with higher GPA on average males earn more than felames, so __iii__ is correct.

__(b)__


\[\hat{y} = 50 + 20\times 4 + 0.07\times 110 + 35 + 0.01(4\times 110) -10\times 4\] 

\[Salary = 137.1\]

__(c)__

*$False$. We could test the hypothesis $H_0 : \hat{\beta_4} = 0$ to validate whether the $GPA/IQ$ has an impact on the quality of the model that also includes observing the $p-value$ associated with the observed value of $t$ or $F$.*

### Problem 4:

__(a)__

*Compare to linear fit, in general, the polynomial regression has lower train RSS because of its higher flexibility. Therefore, the $cubic$ model would fit the data better with lower training RSS.*   

__(b)__

*In this scenario, the test RSS will depend on test data, however, we don't have enough information to conclude. But assuming polynomial regression with higher  test RSS would overfit the training with more error compare to linear regression.*

__(c)__


*Compare to linear fit, in general, the polynomial regression has lower train RSS because of its higher flexibility, which also could explain that regardless of the underlying relationship; the more flexible the model the reduced training RSS will be and that would also allow the points to follow closely.* 


__(d)__

*Again insufficient information to conclude if the test RSS for linear is lower than the cubic and vice versa. Exactly we don't know how far it is from linear, but in general, the $linear$ test RSS could be lower than the $cubic$ test RSS if it closer to linear than cubic and vice versa.*




